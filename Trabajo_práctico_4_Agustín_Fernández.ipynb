{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trabajo práctico 4 - Agustín Fernández",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5387db594654196811db87eb9312e45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_979c61976e824670a75550d7e022e171",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0170a6d85ed14e559d2ed577aae291cc",
              "IPY_MODEL_8a0ede14e9ca4fe68a53e0513e663fd4"
            ]
          }
        },
        "979c61976e824670a75550d7e022e171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0170a6d85ed14e559d2ed577aae291cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_acfb6e1c1dbd46e8817cdfc6f33b78e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a83faa5e87974d1b890e5da856ef488e"
          }
        },
        "8a0ede14e9ca4fe68a53e0513e663fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2290c550037547d0969af5f08cc19c7a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:25&lt;00:00, 6614698.71it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_668650ea866f4b41bc394420f0b4fe18"
          }
        },
        "acfb6e1c1dbd46e8817cdfc6f33b78e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a83faa5e87974d1b890e5da856ef488e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2290c550037547d0969af5f08cc19c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "668650ea866f4b41bc394420f0b4fe18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsQDziRvK1Ti"
      },
      "source": [
        "#@title Aprendizaje Profundo | Otoño 2021 by Datitos{display-mode: \"form\" }\n",
        "#@markdown ![71335171.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAIAAAAiOjnJAAACwElEQVR4nOzdMY7iQBBA0WU197/FnJNNJ/FqWvLHZfd7McIGfVVQos3X+/3+A2f7e/UN8EzCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBJfV9/A/7xer6XX3/1/gZ70eU0sEsIiISwSwiIhLBLCIiEsEiP2WEf7m9U9zVnvU9vh85pYJIRFQlgkhEVCWCSERUJYJEbssc5ytL+5at8zec9UM7FICIuEsEgIi4SwSAiLhLBIPGqPdWR1v1VfdwcmFglhkRAWCWGREBYJYZEQFokt9lhHdt4z1UwsEsIiISwSwiIhLBLCIiEsEh/dY+18zq4w7RzlTyYWCWGREBYJYZEQFglhkRAWiS1+jzVhr/Mbd7nP3zCxSAiLhLBICIuEsEgIi4SwSDxqj1U/72r1ulc9l2sCE4uEsEgIi4SwSAiLhLBICIvE7D3W9/fSy6/63dLqdZfvc/F7mMDEIiEsEsIiISwSwiIhLBLCIjF7j3WSs87rTXufyUwsEsIiISwSwiIhLBLCIiEsEq8Ju5N6r3OXc3xP2oeZWCSERUJYJIRFQlgkhEVCWCRG7LGOTN7TfNIdvwcTi4SwSAiLhLBICIuEsEgIi8Toc4U7Pyf9p8n7qiMmFglhkRAWCWGREBYJYZEQFonRe6wjd9zr7MbEIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIiISwSwiIhLBLCIiEsEsIi8S8AAP//HtRtH09JwIEAAAAASUVORK5CYII=)\n",
        "#El siguiente notebook fue traducido por Pablo Marinozi como el cuarto trabajo práctico correspondiente a la versión de Otoño del 2021 del curso Aprendizaje Profundo organizado por Datitos\n",
        "#El trabajo original fue diseñado por los docentes de la CS231n: Convolutional Neural Networks for Visual Recognition de la Universidad de Stanford. http://cs231n.stanford.edu\n",
        "#Para mayor información consultar https://datitos.github.io/curso-aprendizaje-profundo/#calendario"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5gmNsPl3N2b"
      },
      "source": [
        "# Trabajo Práctico N°4: Redes Neuronales Convolucionales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkXbLl91whfS"
      },
      "source": [
        "# Tabla de contenido\n",
        "\n",
        "Esta tarea tiene 4 partes. Vas a aprender PyTorch en **dos niveles diferentes de abstracción**, lo que te ayudará a comprenderlo mejor y a prepararte para el proyecto final.\n",
        "\n",
        "1. **Parte I**: Preparación: utilizaremos el conjunto de datos CIFAR-10.\n",
        "2. **Parte II**: API de Module de PyTorch: **Nivel de abstracción 1**, usaremos `nn.Module` para definir la arquitectura arbitraria de una red neuronal .\n",
        "3. **Parte III**: API de Sequential de PyTorch: **Nivel de abstracción 2**, usaremos `nn.Sequential` para definir muy convenientemente una red de avance en serie .\n",
        "4. **Parte IV**: Desafío abierto de CIFAR-10: implemente su propia red para obtener la mayor precisión posible en CIFAR-10. Puede experimentar con cualquier capa, optimizador, hiperparámetros u otras funciones avanzadas.\n",
        "\n",
        "Aquí hay una tabla de comparación:\n",
        "\n",
        "| API | Flexibilidad | Conveniencia |\n",
        "| --------------- | ------------- | ------------- |\n",
        "| `nn.Module` | Alta | Media |\n",
        "| `nn.Sequential` | Baja | Alta | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K3WcmlbwhfU"
      },
      "source": [
        "# Parte I.Preparación\n",
        "\n",
        "Primero, cargamos el conjunto de datos CIFAR-10. Esto puede tardar un par de minutos la primera vez que lo hagás, pero los archivos deberían permanecer almacenados en caché después de eso.\n",
        "\n",
        "PyTorch proporciona herramientas convenientes para automatizar el proceso de preprocesarlo e iterarlo en minibatches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "F4h9tKaEwhfX"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # useful stateless functions\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfClpLJu8WJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38210f69-2c09-4b2a-e937-c918ed7deca0"
      },
      "source": [
        "  !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -O cifar-10-python.tar.gz\n",
        "  !tar -xzvf cifar-10-python.tar.gz\n",
        "  !rm cifar-10-python.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-02 15:48:26--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  16.3MB/s    in 11s     \n",
            "\n",
            "2021-05-02 15:48:38 (14.4 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "b5387db594654196811db87eb9312e45",
            "979c61976e824670a75550d7e022e171",
            "0170a6d85ed14e559d2ed577aae291cc",
            "8a0ede14e9ca4fe68a53e0513e663fd4",
            "acfb6e1c1dbd46e8817cdfc6f33b78e2",
            "a83faa5e87974d1b890e5da856ef488e",
            "2290c550037547d0969af5f08cc19c7a",
            "668650ea866f4b41bc394420f0b4fe18"
          ]
        },
        "id": "AS7t7M8CwhfY",
        "outputId": "12668b17-986c-4b97-fc09-3650ac0ccec0"
      },
      "source": [
        "NUM_TRAIN = 49000\n",
        "\n",
        "# El paquete torchvision.transforms proporciona herramientas para preprocesar datos\n",
        "# y para realizar el aumento de datos; aquí configuramos una variable transform para\n",
        "# normalizar los datos restando el valor RGB medio y dividiendo por el\n",
        "# desviación estándar de cada valor RGB; hemos \"hardcodeado\" la media y la desviación estándar.\n",
        "transform = T.Compose([\n",
        "                T.ToTensor(),\n",
        "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "            ])\n",
        "\n",
        "# Configuramos un objeto Dataset para cada split (train / val / test); torchvision.datasets carga\n",
        "# ejemplos de entrenamiento uno a la vez, por lo que empaquetamos cada conjunto de datos en un DataLoader que\n",
        "# itera a través del conjunto de datos y forma minibatches. Dividimos el conjunto de entrenamiento de CIFAR-10\n",
        "# en los conjuntos train y val pasando un objeto Sampler al DataLoader indicando cómo debe muestrear el conjunto de datos subyacente.\n",
        "cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
        "                             transform=transform)\n",
        "loader_train = DataLoader(cifar10_train, batch_size=64, \n",
        "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
        "\n",
        "cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n",
        "                           transform=transform)\n",
        "loader_val = DataLoader(cifar10_val, batch_size=64, \n",
        "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
        "\n",
        "cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True, \n",
        "                            transform=transform)\n",
        "loader_test = DataLoader(cifar10_test, batch_size=64)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cs231n/datasets/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5387db594654196811db87eb9312e45",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./cs231n/datasets/cifar-10-python.tar.gz to ./cs231n/datasets\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-ignore"
        ],
        "id": "tUZo6WPKwhfZ"
      },
      "source": [
        "Tenés la opción de **usar GPU configurando la variable `USE_GPU` en True en la celda de abajo**. No es estrictamente necesario utilizar GPU para este práctico, aunque habilitarla ocasionará que el entrenamiento demore menos. Tené en cuenta que si tu computadora no tiene CUDA habilitado, `torch.cuda.is_available ()` devolverá False y este notebook regresará al modo CPU.\n",
        "\n",
        "Las variables globales `dtype` y` device` controlarán los tipos de datos a lo largo de este práctico.\n",
        "\n",
        "## Usuarios de Colab\n",
        "\n",
        "Si estás utilizando Colab, tenés que cambiar manualmente a un dispositivo GPU. Podés hacer esto haciendo clic en `Entorno de ejecución -> Cambiar tipo de entorno de ejecución` y seleccionando` GPU` en `Acelerador de hardware`. Tené en cuenta que tenés que volver a ejecutar las celdas desde arriba, porque el kernel se reinicia al cambiar los entornos de ejecución."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore-input"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_P5xHYSwhfb",
        "outputId": "b5027e8e-5520-4f3e-a4b1-e328d32dea91"
      },
      "source": [
        "USE_GPU = True\n",
        "\n",
        "dtype = torch.float32 # vamos a usar float a lo largo de este tutorial\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# Constante para controlar la frecuencia con la que imprimimos la pérdida durante el entrenamiento\n",
        "print_every = 100\n",
        "\n",
        "print('using device:', device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWejmW9mwhfx"
      },
      "source": [
        "# Parte III. API Module de PyTorch\n",
        "\n",
        "PyTorch proporciona la API `nn.Module` para que definas arquitecturas de red arbitrarias, mientras realizás un seguimiento de todos los parámetros aprendibles. PyTorch también proporciona el paquete `torch.optim` que implementa todos los optimizadores comunes, como RMSProp, Adagrad y Adam.\n",
        "\n",
        "Para utilizar la API Module, siga los pasos a continuación:\n",
        "\n",
        "1. Genere una subclase de `nn.Module`. Dé a su clase de red un nombre intuitivo como \"2CapasMLP\".\n",
        "\n",
        "2. En el constructor `__init __ ()`, defina todas las capas que necesita como atributos de clase. Los objetos de capa como `nn.Linear` y` nn.Conv2d` son en sí mismos subclases de `nn.Module` y contienen parámetros que se pueden aprender, por lo que no tenés que crear una instancia para esos tensores. `nn.Module` monitoreará estos parámetros internos por vos. Consultá la [documentación] (http://pytorch.org/docs/master/nn.html) para obtener más información sobre las docenas de capas integradas en Pytorch. ** Advertencia **: ¡no te olvidés dellamar al `super () .__ init __ ()` primero!**\n",
        "\n",
        "3. En el método `forward()`, definí la *conectividad* de tu red. Tenés usar los atributos definidos en `__init__` como llamadas de función que toman el tensor como entrada y generan como salida al tensor \"transformado\". **No** creés capas nuevas con parámetros que se puedan aprender en `forward()`! Todos ellos tienen que declararse por adelantado en `__init__`.\n",
        "\n",
        "Después de definir la subclase de Module, podés instanciarla como un objeto y llamarla como una función.\n",
        "\n",
        "### API Module: MLP de dos capas\n",
        "A continuación, se muestra un ejemplo concreto de un MLP de dos capas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "pdf-ignore-input"
        ],
        "id": "NAGJ6z-_3QNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86621359-01dd-4a6d-ecd8-86579c06231b"
      },
      "source": [
        "def flatten(x):\n",
        "    N = x.shape[0] # lee un tensor con dimensiones N, C, H, W\n",
        "    return x.view(N, -1)  # \"aplana\" las componentes C * H * W ven un solo vector por imagen\n",
        "\n",
        "def test_flatten():\n",
        "    x = torch.arange(12).view(2, 1, 3, 2)\n",
        "    print('Antes de flatten: ', x,'\\n')\n",
        "    print('Después de flatten: ', flatten(x))\n",
        "\n",
        "test_flatten()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Antes de flatten:  tensor([[[[ 0,  1],\n",
            "          [ 2,  3],\n",
            "          [ 4,  5]]],\n",
            "\n",
            "\n",
            "        [[[ 6,  7],\n",
            "          [ 8,  9],\n",
            "          [10, 11]]]]) \n",
            "\n",
            "Después de flatten:  tensor([[ 0,  1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10, 11]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXfmAtVwwhfx",
        "outputId": "773492e1-d2f8-4544-8b1f-a2a119efe072"
      },
      "source": [
        "class DosCapasMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        # asigna instancias de capas a los atributos de la clase\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        # nn.init contiene métodos de inicialización convenientes\n",
        "        # http://pytorch.org/docs/master/nn.html#torch-nn-init \n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # forward siempre define la conectividad\n",
        "        x = flatten(x)\n",
        "        scores = self.fc2(F.relu(self.fc1(x)))\n",
        "        return scores\n",
        "\n",
        "def test_DosCapasMLP():\n",
        "    input_size = 50\n",
        "    x = torch.zeros((64, input_size), dtype=dtype)  # minibatch size 64, feature dimension 50\n",
        "    model = DosCapasMLP(input_size, 42, 10)\n",
        "    scores = model(x)\n",
        "    print(scores.size())  # deberías ver [64, 10]\n",
        "test_DosCapasMLP()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkbpEu-Rwhfy"
      },
      "source": [
        "###API Module: CNN de 3 Capas\n",
        "Es tu turno de implementar una CNN de 3 capas seguida de una capa densa. La arquitectura de la red debe ser la siguiente:\n",
        "\n",
        "1. Capa convolucional con una cantidad `channel_1` de filtros de 5x5 con padding=2\n",
        "2. ReLU\n",
        "3. Capa convolucional con una cantidad `channel_2` de filtros 3x3 con padding=1\n",
        "4. ReLU\n",
        "5. Capa densa con una cantidad `num_classes` de neuronas de salida.\n",
        "\n",
        "Tenés que inicializar las matrices de peso del modelo utilizando el método de inicialización normal de Kaiming.\n",
        "\n",
        "**PISTA**: http://pytorch.org/docs/stable/nn.html#conv2d\n",
        "\n",
        "Después de implementar la CNN de tres capas, la función `test_TresCapasCNN` ejecutará su implementación; debería imprimir `(64, 10)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "module_output_shape",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92825c6c-d81a-443b-b021-89364e154a2d"
      },
      "source": [
        "class TresCapasCNN(nn.Module):\n",
        "    def __init__(self, in_channel, channel_1, channel_2, num_classes):\n",
        "        super().__init__()\n",
        "        ########################################################################\n",
        "        # TO_DO: Configure las capas que necesita para la CNN de tres capas con#\n",
        "        # la arquitectura definida anteriormente.                              #\n",
        "        ########################################################################\n",
        "        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n",
        "\n",
        "        self.capa1 = nn.Conv2d(in_channel, channel_1, kernel_size=5, padding=2)\n",
        "        nn.init.kaiming_normal_(self.capa1.weight)\n",
        "        self.capa2 =nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1)\n",
        "        nn.init.kaiming_normal_(self.capa2.weight)\n",
        "        self.flatten =  nn.Flatten()\n",
        "        self.salida = nn.Linear(32*32*channel_2, 10)\n",
        "        nn.init.kaiming_normal_(self.salida.weight)\n",
        "\n",
        "        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n",
        "        ########################################################################\n",
        "        #                          FINAL DE TU CÓDIGO                          #       \n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = None\n",
        "        ########################################################################\n",
        "        # TODO: Implementá la función forward para la CNN de 3 capas. Deberías #\n",
        "        # usar las capas que definiste en __init__ y especificar la            #\n",
        "        # conectividad de dichas capas                                         #\n",
        "        ########################################################################\n",
        "        # ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n",
        "\n",
        "        scores = F.relu(self.capa2(F.relu(self.capa1(x))))\n",
        "        scores = self.salida(self.flatten(scores))\n",
        "\n",
        "        # ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****\n",
        "        ########################################################################\n",
        "        #                          FINAL DE TU CÓDIGO                          #       \n",
        "        ########################################################################\n",
        "        return scores\n",
        "\n",
        "\n",
        "def test_TresCapasCNN():\n",
        "    x = torch.zeros((64, 3, 32, 32), dtype=dtype)  # minibatch size 64, image size [3, 32, 32]\n",
        "    model = TresCapasCNN(in_channel=3, channel_1=12, channel_2=8, num_classes=10)\n",
        "    scores = model(x)\n",
        "    print(scores.size())  # deberías ver [64, 10]\n",
        "test_TresCapasCNN()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0FJhB65whf0"
      },
      "source": [
        "### Module API: Calcular Accuracy\n",
        "Dado el conjunto de pruebas o de validación, podemos verificar el accuracy de clasificación de una red neuronal.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riYGuGXHwhf1"
      },
      "source": [
        "def check_accuracy_part34(loader, model):\n",
        "    if loader.dataset.train:\n",
        "        print('Checking accuracy on validation set')\n",
        "    else:\n",
        "        print('Checking accuracy on test set')   \n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()  # set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "            scores = model(x)\n",
        "            _, preds = scores.max(1)\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_samples += preds.size(0)\n",
        "        acc = float(num_correct) / num_samples\n",
        "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbncEWl6whf1"
      },
      "source": [
        "### Module API: Bucle de entrenamiento\n",
        "En lugar de actualizar los valores de los pesos nosotros mismos, utilizamos un objeto Optimizer del paquete `torch.optim`, que abstrae la noción de un algoritmo de optimización y proporciona implementaciones de la mayoría de los algoritmos comúnmente utilizados para esta tarea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jv-hTALwhf3"
      },
      "source": [
        "def train_part34(model, optimizer, epochs=1):\n",
        "    \"\"\"\n",
        "    Entrena un modelo en CIFAR-10 usando la API Module de PyTorch.\n",
        "\n",
        "    Entradas:\n",
        "    - model: Un Module de PyTorch que da el modelo a entrenar.\n",
        "    - optimizer: un objeto Optimizador que usaremos para entrenar el modelo\n",
        "    - epochs: (Opcional) Un número entero de Python que da el número de épocas para entrenar\n",
        "\n",
        "    Devuelve: nada, pero imprime la precisión del modelo durante el entrenamiento.\n",
        "    \"\"\" \n",
        "    model = model.to(device=device)  # mueve los parámetros del modelo to CPU/GPU\n",
        "    for e in range(epochs):\n",
        "        print('Epoch %d' % (e))\n",
        "        for t, (x, y) in enumerate(loader_train):\n",
        "            model.train()  # pone el modelo en modo entrenamiento\n",
        "            x = x.to(device=device, dtype=dtype)  # mover a device, e.g. GPU\n",
        "            y = y.to(device=device, dtype=torch.long)\n",
        "\n",
        "            scores = model(x)\n",
        "            loss = F.cross_entropy(scores, y)\n",
        "\n",
        "            # Pone a cero todos los gradientes de las variables que el optimizador\n",
        "            # actualizará.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Este es el paso de propagación hacia atrás: calcula el gradiente \n",
        "            # de la función de pérdida con respecto a cada parámetro del modelo.\n",
        "            loss.backward()\n",
        "\n",
        "            # Efectivamente actualiza los parámetros del modelo usando los gradientes\n",
        "            # calculado por el paso anterior\n",
        "            optimizer.step()\n",
        "\n",
        "            if t % print_every == 0:\n",
        "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
        "                check_accuracy_part34(loader_val, model)\n",
        "                print()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLWRWRJCwhf3"
      },
      "source": [
        "### API Module: Entrenamiento de un MLP de dos capas\n",
        "Ahora estamos listos para ejecutar el ciclo de entrenamiento. \n",
        "\n",
        "Simplemente pasá el tamaño de la entrada, el tamaño de la capa oculta y el número de clases (es decir, el tamaño de salida) al constructor de `DosCapasMLP`.\n",
        "\n",
        "También necesitas definir un optimizador que monitoree todos los parámetros que se pueden aprender dentro de `DosCapasMLP`.\n",
        "\n",
        "No tenés que ajustar ningún hiperparámetro, pero deberías obtener un accuracy del modelo por encima del 40% después de una sola época de entrenamiento ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBDCm9PQwhf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749df3a2-2f4a-4323-98fe-8b96094628e1"
      },
      "source": [
        "hidden_layer_size = 4000\n",
        "learning_rate = 1e-2\n",
        "model = DosCapasMLP(3 * 32 * 32, hidden_layer_size, 10)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_part34(model, optimizer)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Iteration 0, loss = 3.7353\n",
            "Checking accuracy on validation set\n",
            "Got 161 / 1000 correct (16.10)\n",
            "\n",
            "Iteration 100, loss = 2.5313\n",
            "Checking accuracy on validation set\n",
            "Got 312 / 1000 correct (31.20)\n",
            "\n",
            "Iteration 200, loss = 1.9454\n",
            "Checking accuracy on validation set\n",
            "Got 401 / 1000 correct (40.10)\n",
            "\n",
            "Iteration 300, loss = 1.8251\n",
            "Checking accuracy on validation set\n",
            "Got 377 / 1000 correct (37.70)\n",
            "\n",
            "Iteration 400, loss = 1.7568\n",
            "Checking accuracy on validation set\n",
            "Got 383 / 1000 correct (38.30)\n",
            "\n",
            "Iteration 500, loss = 1.9504\n",
            "Checking accuracy on validation set\n",
            "Got 417 / 1000 correct (41.70)\n",
            "\n",
            "Iteration 600, loss = 1.7776\n",
            "Checking accuracy on validation set\n",
            "Got 438 / 1000 correct (43.80)\n",
            "\n",
            "Iteration 700, loss = 1.8119\n",
            "Checking accuracy on validation set\n",
            "Got 429 / 1000 correct (42.90)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcErV7FGwhf4"
      },
      "source": [
        "###API Module: Entrenamiento de una CNN de 3 Capas\n",
        "Ahora deberías usar la API de Module para entrenar una CNN de tres capas en CIFAR. ¡Esto debería ser muy similar al entrenamiento de la red de dos capas! No necesitás ajustar ningún hiperparámetro, pero deberías alcanzar más del 45% después de entrenar durante una época.\n",
        "\n",
        "Deberías entrenar el modelo usando un descenso de gradiente estocástico sin momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "module_accuracy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ff7133-ed3a-49c9-d007-b8e31cdad24c"
      },
      "source": [
        "learning_rate = 3e-3\n",
        "channel_1 = 32\n",
        "channel_2 = 16\n",
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "#############################################################################\n",
        "# TODO: Creá una instancia de TresCapasCNN y un optimizador correspondiente #\n",
        "#############################################################################\n",
        "# ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n",
        "\n",
        "model = TresCapasCNN(3,channel_1, channel_2, 10)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum = 0)\n",
        "\n",
        "# ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****       #\n",
        "#############################################################################\n",
        "#                          FINAL DE TU CÓDIGO                               #       \n",
        "#############################################################################\n",
        "\n",
        "train_part34(model, optimizer)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Iteration 0, loss = 2.9369\n",
            "Checking accuracy on validation set\n",
            "Got 122 / 1000 correct (12.20)\n",
            "\n",
            "Iteration 100, loss = 1.7666\n",
            "Checking accuracy on validation set\n",
            "Got 315 / 1000 correct (31.50)\n",
            "\n",
            "Iteration 200, loss = 1.9538\n",
            "Checking accuracy on validation set\n",
            "Got 391 / 1000 correct (39.10)\n",
            "\n",
            "Iteration 300, loss = 1.9326\n",
            "Checking accuracy on validation set\n",
            "Got 380 / 1000 correct (38.00)\n",
            "\n",
            "Iteration 400, loss = 1.3583\n",
            "Checking accuracy on validation set\n",
            "Got 442 / 1000 correct (44.20)\n",
            "\n",
            "Iteration 500, loss = 1.6253\n",
            "Checking accuracy on validation set\n",
            "Got 446 / 1000 correct (44.60)\n",
            "\n",
            "Iteration 600, loss = 1.3650\n",
            "Checking accuracy on validation set\n",
            "Got 474 / 1000 correct (47.40)\n",
            "\n",
            "Iteration 700, loss = 1.3943\n",
            "Checking accuracy on validation set\n",
            "Got 482 / 1000 correct (48.20)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkgdRb8cwhf5"
      },
      "source": [
        "# Part III. API Sequential de Pytorch\n",
        "\n",
        "La Parte II presentó la API Module de PyTorch, que le permite definir capas de aprendizaje arbitrarias y su conectividad.\n",
        "\n",
        "Para modelos simples como una serie de capas apiladas, aún necesita seguir 3 pasos: generar una subclase de `nn.Module`, asignar capas a atributos de clase en` __init__`, y llamar a cada capa una por una en `forward ()`. ¿Existe una forma más conveniente?\n",
        "\n",
        "Afortunadamente, PyTorch proporciona un contenedor subclase de Module llamado `nn.Sequential`, que fusiona los pasos anteriores en uno. No es tan flexible como `nn.Module`, porque no puede especificar una topología más compleja que una serie de capas apiladas, pero es lo suficientemente bueno para muchos casos de uso.\n",
        "\n",
        "### API Sequential: MLP de Dos Capas\n",
        "Veamos cómo reescribir nuestro ejemplo de MLP de dos capas con `nn.Sequential`, y entrenarlo usando el ciclo de entrenamiento definido anteriormente.\n",
        "\n",
        "Nuevamente, no tenés que ajustar ningún hiperparámetro acá, pero deberías lograr una precisión superior al 40% después de una época de entrenamiento. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxo5bVSowhf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "483a43ee-80f5-491c-982c-5023e4e27594"
      },
      "source": [
        "# Necesitamos envolver la función `flatten` en un Module para apilarlo\n",
        "# en nn.Sequential\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return flatten(x)\n",
        "\n",
        "hidden_layer_size = 4000\n",
        "learning_rate = 1e-2\n",
        "\n",
        "model = nn.Sequential(\n",
        "    Flatten(),\n",
        "    nn.Linear(3 * 32 * 32, hidden_layer_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_layer_size, 10),\n",
        ")\n",
        "\n",
        "# podés utilizar el momentum de Nesterov en optim.SGD\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                     momentum=0.9, nesterov=True)\n",
        "\n",
        "train_part34(model, optimizer)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Iteration 0, loss = 2.3586\n",
            "Checking accuracy on validation set\n",
            "Got 156 / 1000 correct (15.60)\n",
            "\n",
            "Iteration 100, loss = 2.0532\n",
            "Checking accuracy on validation set\n",
            "Got 386 / 1000 correct (38.60)\n",
            "\n",
            "Iteration 200, loss = 1.8986\n",
            "Checking accuracy on validation set\n",
            "Got 399 / 1000 correct (39.90)\n",
            "\n",
            "Iteration 300, loss = 1.7846\n",
            "Checking accuracy on validation set\n",
            "Got 416 / 1000 correct (41.60)\n",
            "\n",
            "Iteration 400, loss = 1.9209\n",
            "Checking accuracy on validation set\n",
            "Got 409 / 1000 correct (40.90)\n",
            "\n",
            "Iteration 500, loss = 1.8483\n",
            "Checking accuracy on validation set\n",
            "Got 433 / 1000 correct (43.30)\n",
            "\n",
            "Iteration 600, loss = 1.7829\n",
            "Checking accuracy on validation set\n",
            "Got 417 / 1000 correct (41.70)\n",
            "\n",
            "Iteration 700, loss = 1.6395\n",
            "Checking accuracy on validation set\n",
            "Got 448 / 1000 correct (44.80)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A55hF-vswhf7"
      },
      "source": [
        "### API Sequential: CNN de 3 capas\n",
        "Acá tenés que usar `nn.Sequential` para definir y entrenar una CNN de tres capas con la misma arquitectura que usamos en la Parte II:\n",
        "\n",
        "1. Capa convolucional (con sesgo) con 32 filtros de 5x5, con padding de 2\n",
        "2. ReLU\n",
        "3. Capa convolucional (con sesgo) con 16 filtros 3x3, con padding de 1\n",
        "4. ReLU\n",
        "5. Capa densa (con sesgo) para calcular las puntuaciones de 10 clases\n",
        "\n",
        "Tenés que optimizar tu modelo usando el descenso de gradiente estocástico con el momentum de Nesterov en 0.9.\n",
        "\n",
        "De nuevo, no necesitás ajustar ningún hiperparámetro, pero deberías ver un accuracy superior al 55% después de una época de entrenamiento. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sequential_accuracy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc1cb329-5f90-4841-dfd1-e7267daf3187"
      },
      "source": [
        "channel_1 = 32\n",
        "channel_2 = 16\n",
        "learning_rate = 1e-2\n",
        "\n",
        "model = None\n",
        "optimizer = None\n",
        "\n",
        "#############################################################################\n",
        "# TODO: Reescribí la CNN de 3 capas con sesgo de la Parte II con la         #\n",
        "# API Sequential.                                                           #\n",
        "#############################################################################\n",
        "# ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n",
        "\n",
        "in_c = 3\n",
        "out = 10\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(in_c, channel_1, kernel_size=5, padding=2), \n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(channel_1, channel_2, kernel_size=3, padding=1), \n",
        "    nn.ReLU(),\n",
        "    Flatten(),\n",
        "    nn.Linear(32*32*channel_2, out),\n",
        ")\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                     momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "# ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****       #\n",
        "#############################################################################\n",
        "#                          FINAL DE TU CÓDIGO                               #       \n",
        "#############################################################################\n",
        "\n",
        "train_part34(model, optimizer)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Iteration 0, loss = 2.3086\n",
            "Checking accuracy on validation set\n",
            "Got 96 / 1000 correct (9.60)\n",
            "\n",
            "Iteration 100, loss = 1.4128\n",
            "Checking accuracy on validation set\n",
            "Got 405 / 1000 correct (40.50)\n",
            "\n",
            "Iteration 200, loss = 1.4094\n",
            "Checking accuracy on validation set\n",
            "Got 489 / 1000 correct (48.90)\n",
            "\n",
            "Iteration 300, loss = 1.7440\n",
            "Checking accuracy on validation set\n",
            "Got 507 / 1000 correct (50.70)\n",
            "\n",
            "Iteration 400, loss = 1.1580\n",
            "Checking accuracy on validation set\n",
            "Got 522 / 1000 correct (52.20)\n",
            "\n",
            "Iteration 500, loss = 1.1095\n",
            "Checking accuracy on validation set\n",
            "Got 566 / 1000 correct (56.60)\n",
            "\n",
            "Iteration 600, loss = 1.2839\n",
            "Checking accuracy on validation set\n",
            "Got 565 / 1000 correct (56.50)\n",
            "\n",
            "Iteration 700, loss = 1.1923\n",
            "Checking accuracy on validation set\n",
            "Got 565 / 1000 correct (56.50)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY95QlZewhf8"
      },
      "source": [
        "# Parte III. Desafío abierto de CIFAR-10\n",
        "\n",
        "En esta sección, podés experimentar con cualquier arquitectura CNN que desee en CIFAR-10.\n",
        "\n",
        "Ahora es tu trabajo experimentar con arquitecturas, hiperparámetros, funciones de pérdida y optimizadores para entrenar un modelo que logre ** al menos un 70% ** de accuracy en el conjunto de validación de CIFAR-10 en 10 épocas. Podés utilizar las funciones check_accuracy y train que ya definimos más arriba. Podés utilizar la API `nn.Module` o` nn.Sequential`.\n",
        "\n",
        "Describí lo que hiciste al final de este notebook.\n",
        "\n",
        "Acá está la documentación oficial de la API para cada componente. \n",
        "\n",
        "* Capas en el paquete torch.nn: http://pytorch.org/docs/stable/nn.html\n",
        "* Activaciones: http://pytorch.org/docs/stable/nn.html#non-linear-activations\n",
        "* Funciones de pérdida: http://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "* Optimizadores: http://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "\n",
        "### Cosas que podrías probar:\n",
        "- ** Tamaño de kernel **: arriba usamos 5x5; ¿Serían más eficientes los kernels más pequeños?\n",
        "- ** Número de kernels **: arriba usamos 32 kernels. ¿Más o menos lo hacen mejor?\n",
        "- ** Pooling vs Strided Convolution **: ¿Utilizas el pooling máximo o simplemente convoluciones con stride?\n",
        "- ** Normalización por lotes **: intente agregar la normalización por lotes espacial después de las capas de convolución ¿Tus redes se entrenan más rápido?\n",
        "- ** Arquitectura de red **: la red anterior tiene dos capas de parámetros entrenables. ¿Puede hacerlo mejor con una red profunda? Las buenas arquitecturas para probar incluyen:\n",
        "    - [conv-relu-pool]xN -> [afín]xM -> [softmax o SVM]\n",
        "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax o SVM]\n",
        "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax o SVM]\n",
        "- ** Pooling promedio global **: en lugar de aplanar y luego tener varias capas afines, realice convoluciones hasta que su imagen se vuelva pequeña (7x7 más o menos) y luego realice una operación de pooling promedio para obtener una imagen de imagen 1x1 (1, 1, Filter #), que luego se transforma en un vector (Filter #). Esto se utiliza en la [Red Inception de Google] (https://arxiv.org/abs/1512.00567) (consulte la Tabla 1 para conocer su arquitectura).\n",
        "- ** Regularización **: agregue regularización de pesos, o tal vez Dropout.\n",
        "\n",
        "### Consejos para el entrenamiento\n",
        "Para cada arquitectura de red que pruebe, debe ajustar la tasa de aprendizaje y otros hiperparámetros. Al hacer esto, hay un par de cosas importantes a tener en cuenta:\n",
        "\n",
        "- Si los hiperparámetros funcionan bien, deberías ver una mejora en unos pocos cientos de iteraciones\n",
        "- Recordá el enfoque de grueso a fino para el ajuste de hiperparámetros: comenzá probando una amplia gama de hiperparámetros para solo unas pocas iteraciones de entrenamiento para encontrar las combinaciones de parámetros que funcionan.\n",
        "- Una vez que hayas encontrado algunos conjuntos de hiperparámetros que parecen funcionar, buscá con más precisión alrededor de estos hiperparámetros. Puede que tengas que entrenar para más épocas.\n",
        "- Tenés que utilizar el conjunto de validación para la búsqueda de hiperparámetros y guardarte el conjunto de prueba para evaluar tu arquitectura con los mejores parámetros seleccionados por el conjunto de validación.\n",
        "\n",
        "### Ir más allá\n",
        "Si te sentís aventurero, hay muchas otras features que podés implementar para intentar mejorar tu rendimiento. ** No es necesario ** que implementes ninguno de estos, ¡pero no te perdás la diversión si tenés tiempo!\n",
        "\n",
        "- Optimizadores alternativos: podés probar Adam, Adagrad, RMSprop, etc.\n",
        "- Funciones de activación alternativas como Leaky ReLU, PReLU, ELU o MaxOut.\n",
        "- Aumento de datos\n",
        "- Nuevas arquitecturas\n",
        "  - [ResNets] (https://arxiv.org/abs/1512.03385) donde la entrada de la capa anterior se agrega a la salida.\n",
        "  - [DenseNets] (https://arxiv.org/abs/1608.06993) donde las entradas de las capas anteriores se concatenan juntas.\n",
        "  - [Este blog tiene una descripción detallada] (https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
        "\n",
        "### ¡Divertite y feliz entrenamiento!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "open_ended_accuracy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3e474c-c251-4db4-bf5c-9beb4e82e2f3"
      },
      "source": [
        "################################################################################\n",
        "# TODO:                                                                        #         \n",
        "# Experimentá con arquitecturas, optimizadores e hiperparámetros.              #\n",
        "# Logre AL MENOS 70% de accuracy en el conjunto de validación en 10 épocas.    #\n",
        "#                                                                              #\n",
        "# Tené en cuenta que podés usar la función check_accuracy para evaluar tanto   #\n",
        "# el conjunto de prueba como el conjunto de validación, pasando loader_test o  #\n",
        "# loader_val como segundo argumento para check_accuracy. No deberías tocar el  #\n",
        "# conjunto de prueba hasta que hayas elegido tu arquitectura e hiperparámetros.#\n",
        "# Solo usás el conjunto de prueba una vez para informar un puntaje al final    #\n",
        "################################################################################\n",
        "model = None\n",
        "optimizer = None\n",
        "#############################################################################\n",
        "# ***** INICIO DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) ***** \n",
        "\n",
        "\n",
        "class Modelito(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel, channel_1, channel_2, channel_3, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.capa1 = nn.Conv2d(in_channel, channel_1, kernel_size = 3, padding = 1)\n",
        "        nn.init.kaiming_normal_(self.capa1.weight)\n",
        "        self.capa2 = nn.Conv2d(channel_1, channel_2, kernel_size = 3, padding = 1)\n",
        "        nn.init.kaiming_normal_(self.capa2.weight)\n",
        "        self.capa3 = nn.Conv2d(channel_2, channel_3, kernel_size = 3, padding = 1)\n",
        "        nn.init.kaiming_normal_(self.capa3.weight)\n",
        "        self.salida = nn.Linear(16 * 4* 4, num_classes)\n",
        "        nn.init.kaiming_normal_(self.salida.weight)\n",
        "        self.dropout1 = nn.Dropout2d(0.2)\n",
        "        self.flatten =  nn.Flatten()\n",
        "        self.batchnorm1 = nn.BatchNorm2d(channel_1)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(channel_2)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(channel_3)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.relu(self.batchnorm1(self.capa1(x)))\n",
        "\n",
        "        \n",
        "        x = F.max_pool2d( x ,kernel_size = 3, padding =1)\n",
        "\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = F.relu(self.batchnorm2(self.capa2(x)))\n",
        "\n",
        "        x = F.max_pool2d(x ,kernel_size = 3, padding =1)\n",
        "\n",
        "        x = self.dropout1(x)\n",
        "        \n",
        "        x = F.relu(self.batchnorm3(self.capa3(x)))\n",
        "       \n",
        "        \n",
        "        x =  self.salida(self.flatten(x))\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "in_channel = 3\n",
        "channel_1 = 64 \n",
        "channel_2 = 32\n",
        "channel_3 = 16\n",
        "num_classes = 10\n",
        "learning_rate = 1e-2\n",
        "\n",
        "model = Modelito(in_channel, channel_1 , channel_2 , channel_3 , num_classes )\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
        "# ***** FINAL DE TU CÓDIGO (NO BORRES NI MODIFIQUES ESTA LÍNEA) *****       #\n",
        "#############################################################################\n",
        "#                          FINAL DE TU CÓDIGO                               #       \n",
        "#############################################################################\n",
        "\n",
        "# Tenés que llegar a un 70% de accuracy\n",
        "train_part34(model, optimizer, epochs=10)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Iteration 0, loss = 2.4675\n",
            "Checking accuracy on validation set\n",
            "Got 128 / 1000 correct (12.80)\n",
            "\n",
            "Iteration 100, loss = 1.6889\n",
            "Checking accuracy on validation set\n",
            "Got 426 / 1000 correct (42.60)\n",
            "\n",
            "Iteration 200, loss = 1.6153\n",
            "Checking accuracy on validation set\n",
            "Got 494 / 1000 correct (49.40)\n",
            "\n",
            "Iteration 300, loss = 1.7208\n",
            "Checking accuracy on validation set\n",
            "Got 493 / 1000 correct (49.30)\n",
            "\n",
            "Iteration 400, loss = 1.6439\n",
            "Checking accuracy on validation set\n",
            "Got 524 / 1000 correct (52.40)\n",
            "\n",
            "Iteration 500, loss = 1.6531\n",
            "Checking accuracy on validation set\n",
            "Got 542 / 1000 correct (54.20)\n",
            "\n",
            "Iteration 600, loss = 1.3639\n",
            "Checking accuracy on validation set\n",
            "Got 548 / 1000 correct (54.80)\n",
            "\n",
            "Iteration 700, loss = 1.2675\n",
            "Checking accuracy on validation set\n",
            "Got 568 / 1000 correct (56.80)\n",
            "\n",
            "Epoch 1\n",
            "Iteration 0, loss = 1.1402\n",
            "Checking accuracy on validation set\n",
            "Got 549 / 1000 correct (54.90)\n",
            "\n",
            "Iteration 100, loss = 1.3347\n",
            "Checking accuracy on validation set\n",
            "Got 589 / 1000 correct (58.90)\n",
            "\n",
            "Iteration 200, loss = 1.3007\n",
            "Checking accuracy on validation set\n",
            "Got 540 / 1000 correct (54.00)\n",
            "\n",
            "Iteration 300, loss = 1.3532\n",
            "Checking accuracy on validation set\n",
            "Got 582 / 1000 correct (58.20)\n",
            "\n",
            "Iteration 400, loss = 1.4357\n",
            "Checking accuracy on validation set\n",
            "Got 605 / 1000 correct (60.50)\n",
            "\n",
            "Iteration 500, loss = 1.0886\n",
            "Checking accuracy on validation set\n",
            "Got 599 / 1000 correct (59.90)\n",
            "\n",
            "Iteration 600, loss = 1.1484\n",
            "Checking accuracy on validation set\n",
            "Got 598 / 1000 correct (59.80)\n",
            "\n",
            "Iteration 700, loss = 1.1965\n",
            "Checking accuracy on validation set\n",
            "Got 612 / 1000 correct (61.20)\n",
            "\n",
            "Epoch 2\n",
            "Iteration 0, loss = 1.1498\n",
            "Checking accuracy on validation set\n",
            "Got 596 / 1000 correct (59.60)\n",
            "\n",
            "Iteration 100, loss = 1.3782\n",
            "Checking accuracy on validation set\n",
            "Got 607 / 1000 correct (60.70)\n",
            "\n",
            "Iteration 200, loss = 1.2354\n",
            "Checking accuracy on validation set\n",
            "Got 604 / 1000 correct (60.40)\n",
            "\n",
            "Iteration 300, loss = 1.0863\n",
            "Checking accuracy on validation set\n",
            "Got 617 / 1000 correct (61.70)\n",
            "\n",
            "Iteration 400, loss = 1.1959\n",
            "Checking accuracy on validation set\n",
            "Got 617 / 1000 correct (61.70)\n",
            "\n",
            "Iteration 500, loss = 0.9315\n",
            "Checking accuracy on validation set\n",
            "Got 626 / 1000 correct (62.60)\n",
            "\n",
            "Iteration 600, loss = 1.3720\n",
            "Checking accuracy on validation set\n",
            "Got 611 / 1000 correct (61.10)\n",
            "\n",
            "Iteration 700, loss = 1.2409\n",
            "Checking accuracy on validation set\n",
            "Got 669 / 1000 correct (66.90)\n",
            "\n",
            "Epoch 3\n",
            "Iteration 0, loss = 1.0197\n",
            "Checking accuracy on validation set\n",
            "Got 630 / 1000 correct (63.00)\n",
            "\n",
            "Iteration 100, loss = 1.4509\n",
            "Checking accuracy on validation set\n",
            "Got 641 / 1000 correct (64.10)\n",
            "\n",
            "Iteration 200, loss = 1.0944\n",
            "Checking accuracy on validation set\n",
            "Got 645 / 1000 correct (64.50)\n",
            "\n",
            "Iteration 300, loss = 0.9797\n",
            "Checking accuracy on validation set\n",
            "Got 641 / 1000 correct (64.10)\n",
            "\n",
            "Iteration 400, loss = 0.9450\n",
            "Checking accuracy on validation set\n",
            "Got 617 / 1000 correct (61.70)\n",
            "\n",
            "Iteration 500, loss = 1.0519\n",
            "Checking accuracy on validation set\n",
            "Got 661 / 1000 correct (66.10)\n",
            "\n",
            "Iteration 600, loss = 1.1447\n",
            "Checking accuracy on validation set\n",
            "Got 636 / 1000 correct (63.60)\n",
            "\n",
            "Iteration 700, loss = 0.9623\n",
            "Checking accuracy on validation set\n",
            "Got 656 / 1000 correct (65.60)\n",
            "\n",
            "Epoch 4\n",
            "Iteration 0, loss = 1.0528\n",
            "Checking accuracy on validation set\n",
            "Got 642 / 1000 correct (64.20)\n",
            "\n",
            "Iteration 100, loss = 1.0351\n",
            "Checking accuracy on validation set\n",
            "Got 672 / 1000 correct (67.20)\n",
            "\n",
            "Iteration 200, loss = 1.1031\n",
            "Checking accuracy on validation set\n",
            "Got 652 / 1000 correct (65.20)\n",
            "\n",
            "Iteration 300, loss = 1.0246\n",
            "Checking accuracy on validation set\n",
            "Got 645 / 1000 correct (64.50)\n",
            "\n",
            "Iteration 400, loss = 1.0153\n",
            "Checking accuracy on validation set\n",
            "Got 672 / 1000 correct (67.20)\n",
            "\n",
            "Iteration 500, loss = 0.9663\n",
            "Checking accuracy on validation set\n",
            "Got 662 / 1000 correct (66.20)\n",
            "\n",
            "Iteration 600, loss = 0.9769\n",
            "Checking accuracy on validation set\n",
            "Got 661 / 1000 correct (66.10)\n",
            "\n",
            "Iteration 700, loss = 1.0109\n",
            "Checking accuracy on validation set\n",
            "Got 679 / 1000 correct (67.90)\n",
            "\n",
            "Epoch 5\n",
            "Iteration 0, loss = 1.0680\n",
            "Checking accuracy on validation set\n",
            "Got 667 / 1000 correct (66.70)\n",
            "\n",
            "Iteration 100, loss = 1.0184\n",
            "Checking accuracy on validation set\n",
            "Got 674 / 1000 correct (67.40)\n",
            "\n",
            "Iteration 200, loss = 0.8880\n",
            "Checking accuracy on validation set\n",
            "Got 669 / 1000 correct (66.90)\n",
            "\n",
            "Iteration 300, loss = 1.3571\n",
            "Checking accuracy on validation set\n",
            "Got 650 / 1000 correct (65.00)\n",
            "\n",
            "Iteration 400, loss = 1.0010\n",
            "Checking accuracy on validation set\n",
            "Got 670 / 1000 correct (67.00)\n",
            "\n",
            "Iteration 500, loss = 1.0899\n",
            "Checking accuracy on validation set\n",
            "Got 669 / 1000 correct (66.90)\n",
            "\n",
            "Iteration 600, loss = 0.9883\n",
            "Checking accuracy on validation set\n",
            "Got 659 / 1000 correct (65.90)\n",
            "\n",
            "Iteration 700, loss = 0.9927\n",
            "Checking accuracy on validation set\n",
            "Got 674 / 1000 correct (67.40)\n",
            "\n",
            "Epoch 6\n",
            "Iteration 0, loss = 0.9643\n",
            "Checking accuracy on validation set\n",
            "Got 686 / 1000 correct (68.60)\n",
            "\n",
            "Iteration 100, loss = 1.1955\n",
            "Checking accuracy on validation set\n",
            "Got 682 / 1000 correct (68.20)\n",
            "\n",
            "Iteration 200, loss = 1.0784\n",
            "Checking accuracy on validation set\n",
            "Got 683 / 1000 correct (68.30)\n",
            "\n",
            "Iteration 300, loss = 0.9174\n",
            "Checking accuracy on validation set\n",
            "Got 679 / 1000 correct (67.90)\n",
            "\n",
            "Iteration 400, loss = 0.6094\n",
            "Checking accuracy on validation set\n",
            "Got 672 / 1000 correct (67.20)\n",
            "\n",
            "Iteration 500, loss = 0.9471\n",
            "Checking accuracy on validation set\n",
            "Got 681 / 1000 correct (68.10)\n",
            "\n",
            "Iteration 600, loss = 0.9536\n",
            "Checking accuracy on validation set\n",
            "Got 695 / 1000 correct (69.50)\n",
            "\n",
            "Iteration 700, loss = 1.1247\n",
            "Checking accuracy on validation set\n",
            "Got 687 / 1000 correct (68.70)\n",
            "\n",
            "Epoch 7\n",
            "Iteration 0, loss = 1.0319\n",
            "Checking accuracy on validation set\n",
            "Got 689 / 1000 correct (68.90)\n",
            "\n",
            "Iteration 100, loss = 0.9899\n",
            "Checking accuracy on validation set\n",
            "Got 693 / 1000 correct (69.30)\n",
            "\n",
            "Iteration 200, loss = 0.8113\n",
            "Checking accuracy on validation set\n",
            "Got 683 / 1000 correct (68.30)\n",
            "\n",
            "Iteration 300, loss = 1.0809\n",
            "Checking accuracy on validation set\n",
            "Got 693 / 1000 correct (69.30)\n",
            "\n",
            "Iteration 400, loss = 0.9071\n",
            "Checking accuracy on validation set\n",
            "Got 675 / 1000 correct (67.50)\n",
            "\n",
            "Iteration 500, loss = 1.2004\n",
            "Checking accuracy on validation set\n",
            "Got 709 / 1000 correct (70.90)\n",
            "\n",
            "Iteration 600, loss = 1.5090\n",
            "Checking accuracy on validation set\n",
            "Got 687 / 1000 correct (68.70)\n",
            "\n",
            "Iteration 700, loss = 1.0274\n",
            "Checking accuracy on validation set\n",
            "Got 679 / 1000 correct (67.90)\n",
            "\n",
            "Epoch 8\n",
            "Iteration 0, loss = 0.8077\n",
            "Checking accuracy on validation set\n",
            "Got 680 / 1000 correct (68.00)\n",
            "\n",
            "Iteration 100, loss = 0.7253\n",
            "Checking accuracy on validation set\n",
            "Got 688 / 1000 correct (68.80)\n",
            "\n",
            "Iteration 200, loss = 0.8379\n",
            "Checking accuracy on validation set\n",
            "Got 688 / 1000 correct (68.80)\n",
            "\n",
            "Iteration 300, loss = 0.7814\n",
            "Checking accuracy on validation set\n",
            "Got 678 / 1000 correct (67.80)\n",
            "\n",
            "Iteration 400, loss = 0.8905\n",
            "Checking accuracy on validation set\n",
            "Got 699 / 1000 correct (69.90)\n",
            "\n",
            "Iteration 500, loss = 0.9341\n",
            "Checking accuracy on validation set\n",
            "Got 702 / 1000 correct (70.20)\n",
            "\n",
            "Iteration 600, loss = 0.8927\n",
            "Checking accuracy on validation set\n",
            "Got 696 / 1000 correct (69.60)\n",
            "\n",
            "Iteration 700, loss = 1.0494\n",
            "Checking accuracy on validation set\n",
            "Got 700 / 1000 correct (70.00)\n",
            "\n",
            "Epoch 9\n",
            "Iteration 0, loss = 0.9385\n",
            "Checking accuracy on validation set\n",
            "Got 712 / 1000 correct (71.20)\n",
            "\n",
            "Iteration 100, loss = 0.8795\n",
            "Checking accuracy on validation set\n",
            "Got 688 / 1000 correct (68.80)\n",
            "\n",
            "Iteration 200, loss = 0.9136\n",
            "Checking accuracy on validation set\n",
            "Got 710 / 1000 correct (71.00)\n",
            "\n",
            "Iteration 300, loss = 0.8941\n",
            "Checking accuracy on validation set\n",
            "Got 711 / 1000 correct (71.10)\n",
            "\n",
            "Iteration 400, loss = 0.7862\n",
            "Checking accuracy on validation set\n",
            "Got 717 / 1000 correct (71.70)\n",
            "\n",
            "Iteration 500, loss = 0.8674\n",
            "Checking accuracy on validation set\n",
            "Got 712 / 1000 correct (71.20)\n",
            "\n",
            "Iteration 600, loss = 1.1037\n",
            "Checking accuracy on validation set\n",
            "Got 703 / 1000 correct (70.30)\n",
            "\n",
            "Iteration 700, loss = 0.9375\n",
            "Checking accuracy on validation set\n",
            "Got 708 / 1000 correct (70.80)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "pdf-inline"
        ],
        "id": "QQ5bZMHuwhf_"
      },
      "source": [
        "## Describí lo que hiciste\n",
        "\n",
        "En la celda a continuación, tenés que escribir una explicación de lo que hiciste, las funciones adicionales que implementaste y / o los gráficos que realizaste en el proceso de entrenamiento y evaluación de tu red."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHDKlN_52VSj"
      },
      "source": [
        "- En primer lugar decidí construir un modelo basándome en las sugerencias propuestas. El mismo quedó conformado por  3 capas convolucionales y 2 de Max Pooling. A su vez, bajé el número del Kernel de 5x5 a 3x3.\n",
        "\n",
        "- Agregué Batch Normalization después de cada convolución\n",
        "\n",
        "- La estructura quedó conformada como:\n",
        "\n",
        "CV1 - BN - Relu - Max pooling\n",
        "\n",
        "CV2 - BN - Relu - Max Pooling \n",
        "\n",
        "CV3 - BN - Relu\n",
        "\n",
        "-Las capas convolucionales tuvieron de salida 64 canales, 32 canales y 16 canales. Esto implicó agregar más capas a los modelos anteriores.\n",
        "\n",
        "- Agregué Dropout dos veces, luego de realizar los Max pooling\n",
        "\n",
        "-Jugué con la cantidad de dropout y el Learning Rate, siendo las medidas óptimas : \n",
        "learning_rate = 0.01\n",
        "Dropoout = 0.2\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l52Y7_SuwhgB"
      },
      "source": [
        "## Conjunto de prueba: ejecute esto solo una vez\n",
        "\n",
        "Ahora que obtuvimos un resultado con el que estamos contentos, probamos nuestro modelo final en el conjunto de prueba (que debeés almacenar en best_model). Pensá en cómo se compara esto con la precisión de tu conjunto de validación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ8TgtESwhgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f012101-9b22-47c0-ef1c-a51cc93cef9d"
      },
      "source": [
        "best_model = model\n",
        "check_accuracy_part34(loader_test, best_model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking accuracy on test set\n",
            "Got 7006 / 10000 correct (70.06)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQaU2JYjyYWD"
      },
      "source": [
        "Notamos que la precisión en el conjunto de Test casi que no varía en relación a la precisión en el conjunto de Validación. Creemos que esto se debe en gran medida a que el número total de iteraciones en el conjunto de validacióón es bastante parecido al núero total de iteraciones en el conjunto de Test."
      ]
    }
  ]
}